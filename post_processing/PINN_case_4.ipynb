{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2gEWB1YkoaV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr9n_kHak0u0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.axis import Axis\n",
        "import torch.nn.init as init\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import logging\n",
        "from google.colab import drive\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torch.utils.checkpoint as checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZbXwop3kxIf",
        "outputId": "bed563d7-9b22-47b5-93e7-5c6bff785b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzKHqyz2k8uP"
      },
      "outputs": [],
      "source": [
        "log_file = \"/content/drive/MyDrive/NNOutputs/PINN.log\"\n",
        "\n",
        "# Create a custom logger\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Clear previous handlers if re-running in Colab\n",
        "if logger.hasHandlers():\n",
        "    logger.handlers.clear()\n",
        "\n",
        "# Create file handler\n",
        "file_handler = logging.FileHandler(log_file, mode='w')\n",
        "file_handler.setFormatter(\n",
        "    logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s',\n",
        "                      datefmt='%Y-%m-%d %H:%M:%S')\n",
        "    )\n",
        "\n",
        "# Add handler to logger\n",
        "logger.addHandler(file_handler)\n",
        "\n",
        "# Example usage\n",
        "logger.info(\"Logging setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbAT8aFylahF",
        "outputId": "5a37394c-b085-4183-8e9b-00003c065ab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NypOxEbllqgw"
      },
      "outputs": [],
      "source": [
        "def np_to_torch(arr):\n",
        "\n",
        "    arr = torch.FloatTensor(arr)\n",
        "    arr = arr.unsqueeze(-1)\n",
        "    arr = arr.clone().detach().requires_grad_(True)\n",
        "\n",
        "    return arr\n",
        "\n",
        "def xyt_train_data(N_x, x_l, x_r, N_y, y_b, y_t, N_t, t_i, t_f, N_bc, x_c, y_c, r):\n",
        "\n",
        "    start_PCM = [0]\n",
        "    start_HS = [0]\n",
        "\n",
        "    ####  PCM ###\n",
        "    x_train = np.linspace(x_l,x_r,N_x)\n",
        "    x_train = np.tile(x_train,N_y)\n",
        "    y_train = np.linspace(y_b,y_t,N_y)\n",
        "    y_train = np.repeat(y_train,N_x)\n",
        "\n",
        "    n = x_train.shape[0]\n",
        "    dist = np.sqrt( ( x_train - x_c )**2 + ( y_train - y_c )**2 )\n",
        "    phi = (dist>r)\n",
        "    n_PCM = int(np.sum(phi))\n",
        "    phi = np.tile(phi, N_t)\n",
        "\n",
        "    x_train = np.tile(x_train, N_t)\n",
        "    y_train = np.tile(y_train, N_t)\n",
        "\n",
        "    t_train = np.linspace(t_i, t_f, N_t)\n",
        "    t_train = np.repeat(t_train, n)\n",
        "\n",
        "    x_PCM = x_train[phi]\n",
        "    y_PCM = y_train[phi]\n",
        "    t_PCM = t_train[phi]\n",
        "    start_PCM.append( start_PCM[-1] + t_PCM.shape[0])\n",
        "\n",
        "    #### HS ####\n",
        "    x_train = np.linspace(x_l,x_r, int(N_x))\n",
        "    x_train = np.tile(x_train,int(N_y))\n",
        "    y_train = np.linspace(y_b,y_t,int(N_y))\n",
        "    y_train = np.repeat(y_train,int(N_x))\n",
        "\n",
        "    n = x_train.shape[0]\n",
        "    dist = np.sqrt(( x_train - x_c )**2 + ( y_train - y_c )**2)\n",
        "    phi = (dist<=r)\n",
        "    n_HS = int(np.sum(phi))\n",
        "    phi = np.tile(phi, N_t)\n",
        "\n",
        "    x_train = np.tile(x_train, N_t)\n",
        "    y_train = np.tile(y_train, N_t)\n",
        "    dist = np.tile(dist, N_t)\n",
        "\n",
        "    t_train = np.linspace(t_i, t_f, N_t)\n",
        "    t_train = np.repeat(t_train, n)\n",
        "\n",
        "    x_HS = x_train[phi]\n",
        "    y_HS = y_train[phi]\n",
        "    t_HS = t_train[phi]\n",
        "    dist = dist[phi]\n",
        "    start_HS.append( start_HS[-1] + t_HS.shape[0])\n",
        "\n",
        "    x_PCM = np_to_torch(x_PCM)\n",
        "    y_PCM = np_to_torch(y_PCM)\n",
        "    t_PCM = np_to_torch(t_PCM)\n",
        "    x_HS = np_to_torch(x_HS)\n",
        "    y_HS = np_to_torch(y_HS)\n",
        "    t_HS = np_to_torch(t_HS)\n",
        "    # dist = np_to_torch(dist)\n",
        "    dist = torch.FloatTensor(dist)\n",
        "    dist = dist.unsqueeze(-1)\n",
        "\n",
        "    return x_PCM, y_PCM, t_PCM, x_HS, y_HS, t_HS, start_PCM, start_HS, n_PCM, n_HS, dist\n",
        "\n",
        "class RBF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = 0.5*torch.exp(-10*torch.square(x)) - 0.01\n",
        "        return y\n",
        "\n",
        "class Sigm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = 1/( 1 + torch.exp(-80*x) )\n",
        "        return y\n",
        "\n",
        "def xavier_init(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        init.xavier_normal_(m.weight)\n",
        "        # init.xavier_normal_(m.bias)\n",
        "\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self, layer_size_1):\n",
        "        super(ANN, self).__init__()\n",
        "\n",
        "        rbf = RBF()\n",
        "        self.tanh = nn.Tanh()\n",
        "        sigmoid = nn.Sigmoid()\n",
        "        self.act = rbf\n",
        "        mean_init = 0\n",
        "        std_init = 0.707\n",
        "\n",
        "        ########### Fully conected model-1 ###########\n",
        "\n",
        "        modules_11 = []\n",
        "        modules_11.append(nn.Linear(layer_size_1[0], layer_size_1[1]))\n",
        "        modules_11.append(self.act)\n",
        "        modules_11.append(nn.Linear( layer_size_1[1], layer_size_1[2]))\n",
        "        self.fc_11 = nn.Sequential( *modules_11 )\n",
        "\n",
        "        modules_12 = []\n",
        "        modules_12.append(nn.Linear( layer_size_1[1], layer_size_1[2]))\n",
        "        modules_12.append(self.act)\n",
        "        modules_12.append(nn.Linear( layer_size_1[1], layer_size_1[2]))\n",
        "        self.fc_12 = nn.Sequential( *modules_12 )\n",
        "\n",
        "        modules_13 = []\n",
        "        modules_13.append(self.act)\n",
        "        modules_13.append(nn.Linear(layer_size_1[2], layer_size_1[3]))\n",
        "        modules_13.append(self.act)\n",
        "        self.fc_13 = nn.Sequential( *modules_13 )\n",
        "\n",
        "\n",
        "        for module in [self.fc_11, self.fc_12, self.fc_13]:\n",
        "            for layer in module:\n",
        "                if isinstance(layer, nn.Linear):\n",
        "                    init.xavier_normal_(layer.weight)\n",
        "\n",
        "    def forward(self, x_PCM, y_PCM, t_PCM, x_HS, y_HS, t_HS, mode):\n",
        "\n",
        "      ########### Fully conected model-1 ###########\n",
        "\n",
        "        # For PCM branch\n",
        "        out_1 = self.fc_11(torch.cat((x_PCM, y_PCM, t_PCM), 1))\n",
        "        out_2 = self.fc_12(out_1)\n",
        "        T_PCM = self.fc_13(out_1 + out_2)\n",
        "\n",
        "        # For HS branch\n",
        "        out_1 = self.fc_11(torch.cat((x_HS, y_HS, t_HS), 1))\n",
        "        out_2 = self.fc_12(out_1)\n",
        "        T_HS = self.fc_13(out_1 + out_2)\n",
        "\n",
        "        ################ Derivatives ################\n",
        "        if mode == \"train\":\n",
        "            dTdx_PCM = torch.autograd.grad(T_PCM, x_PCM, grad_outputs=torch.ones_like(T_PCM), create_graph=True)[0]\n",
        "            d2Tdx2_PCM = torch.autograd.grad(dTdx_PCM, x_PCM, grad_outputs=torch.ones_like(dTdx_PCM), create_graph=True)[0]\n",
        "            dTdy_PCM = torch.autograd.grad(T_PCM, y_PCM, grad_outputs=torch.ones_like(T_PCM), create_graph=True)[0]\n",
        "            d2Tdy2_PCM = torch.autograd.grad(dTdy_PCM, y_PCM, grad_outputs=torch.ones_like(dTdy_PCM), create_graph=True)[0]\n",
        "            dTdt_PCM = torch.autograd.grad(T_PCM, t_PCM, grad_outputs=torch.ones_like(T_PCM), create_graph=True)[0]\n",
        "\n",
        "            dTdx_HS = torch.autograd.grad(T_HS, x_HS, grad_outputs=torch.ones_like(T_HS), create_graph=True)[0]\n",
        "            d2Tdx2_HS = torch.autograd.grad(dTdx_HS, x_HS, grad_outputs=torch.ones_like(dTdx_HS), create_graph=True)[0]\n",
        "            dTdy_HS = torch.autograd.grad(T_HS, y_HS, grad_outputs=torch.ones_like(T_HS), create_graph=True)[0]\n",
        "            d2Tdy2_HS = torch.autograd.grad(dTdy_HS, y_HS, grad_outputs=torch.ones_like(dTdy_HS), create_graph=True)[0]\n",
        "            dTdt_HS = torch.autograd.grad(T_HS, t_HS, grad_outputs=torch.ones_like(T_HS), create_graph=True)[0]\n",
        "\n",
        "            return  T_PCM, dTdx_PCM, dTdy_PCM, d2Tdx2_PCM, d2Tdy2_PCM, dTdt_PCM, T_HS, dTdx_HS, dTdy_HS, d2Tdx2_HS, d2Tdy2_HS, dTdt_HS\n",
        "\n",
        "        else:\n",
        "\n",
        "            return  T_PCM, 0, 0, 0, 0, 0, T_HS, 0, 0, 0, 0, 0\n",
        "\n",
        "def get_loss(x_PCM, y_PCM, t_PCM, x_HS, y_HS, t_HS, start_PCM, start_HS, k1, k2, k3, w1, w2, w3, w4, w5, w6,\n",
        "             w7, w8, s_norm, n_PCM, n_HS, T_change, N_t, model, source_term):\n",
        "\n",
        "    # with autocast():\n",
        "\n",
        "\n",
        "    T_PCM, dTdx_PCM, dTdy_PCM, d2Tdx2_PCM, d2Tdy2_PCM, dTdt_PCM, T_HS, dTdx_HS, dTdy_HS, d2Tdx2_HS, d2Tdy2_HS, dTdt_HS = model(x_PCM, y_PCM, t_PCM, x_HS, y_HS, t_HS, \"train\")\n",
        "\n",
        "    eq_PCM = w1*( torch.sum( torch.mul( torch.square( dTdt_PCM - k1*(d2Tdx2_PCM + d2Tdy2_PCM) - (6500-2000*x_PCM)*delta_t/(rho*Cp*T_sc).detach()), torch.where((T_PCM >= T_change), 1, 0) ) )\n",
        "    + torch.sum( torch.mul( torch.square( dTdt_PCM - k2*(d2Tdx2_PCM + d2Tdy2_PCM) - (6500-2000*x_PCM)*delta_t/(rho*Cp*T_sc).detach()), torch.where((T_PCM < T_change) & (t_PCM > 0) , 1, 0) ) )\n",
        "    )/(start_HS[-1] + start_PCM[-1])\n",
        "\n",
        "    eq_HS = w2*(torch.sum( torch.mul( torch.square( dTdt_HS - k1*(d2Tdx2_HS + d2Tdy2_HS) - (6500-2000*x_HS)*delta_t/(rho*Cp*T_sc).detach()), torch.where((T_HS >= T_change) & (t_HS > 0), 1, 0) ) )\n",
        "    + torch.sum( torch.mul( torch.square( dTdt_HS - k2*(d2Tdx2_HS + d2Tdy2_HS) - (6500-2000*x_HS)*delta_t/(rho*Cp*T_sc).detach()), torch.where((T_HS < T_change), 1, 0) ) )\n",
        "    )/(start_HS[-1] + start_PCM[-1])\n",
        "\n",
        "    bc1 = w3*torch.sum( torch.mul(torch.square( dTdx_PCM ),\n",
        "                                  torch.where((x_PCM == x_l) & (t_PCM > 0), 1, 0)) )/(N_y*N_t) #left boundary\n",
        "    bc2 = w4*torch.sum( torch.mul(torch.square( dTdy_PCM ),\n",
        "                                  torch.where((y_PCM == y_b) & (t_PCM > 0), 1, 0)) )/(N_x*N_t) #bottom boundary\n",
        "    bc3 = w4*torch.sum( torch.mul(torch.square( dTdy_PCM ),\n",
        "                                  torch.where((y_PCM == y_t) & (t_PCM > 0), 1, 0)) )/(N_x*N_t) #top boundary\n",
        "    bc4 = w6*torch.sum( torch.mul(torch.square( dTdx_PCM ),\n",
        "                              torch.where((x_PCM == x_r) & (t_PCM > 0), 1, 0)) )/(N_y*N_t) #right boundary\n",
        "\n",
        "\n",
        "    ic_PCM = w7*torch.sum( torch.square( T_PCM[0:n_PCM] )  )/(n_PCM)\n",
        "    ic_HS = w8*torch.sum( torch.square( T_HS[0:n_HS] )  )/(n_HS)\n",
        "\n",
        "    loss = eq_PCM + eq_HS + bc1 + bc2 + bc3 + bc4 + ic_PCM + ic_HS\n",
        "\n",
        "    return loss, eq_PCM, eq_HS, bc1, bc2, bc3, bc4, ic_PCM, ic_HS\n",
        "\n",
        "def save_model(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "def load_model(model, path):\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    model.eval()\n",
        "\n",
        "def freeze(model):\n",
        "    for i, param in enumerate(model.parameters()):\n",
        "        print(\"#######\", i, \" #######\" )\n",
        "        if i not in [4, 5, 10, 11]:\n",
        "            param.requires_grad = False\n",
        "        print(param)\n",
        "\n",
        "\n",
        "def plotcheck(x_PCM, y_PCM, t_PCM, x_HS, y_HS, t_HS, start_PCM, start_HS, n_PCM, n_HS, N_t_test, model, T_change, current_dir):\n",
        "\n",
        "    ######################### Compute Results ###########################\n",
        "    model_cpu = model.to('cpu')\n",
        "    model_cpu.eval()\n",
        "    with torch.no_grad():\n",
        "      T_PCM, dTdx_PCM, dTdy_PCM, d2Tdx2_PCM, d2Tdy2_PCM, dTdt_PCM, T_HS, dTdx_HS, dTdy_HS, d2Tdx2_HS, d2Tdy2_HS, dTdt_HS = model_cpu(x_PCM, y_PCM, t_PCM, x_HS, y_HS, t_HS, \"eval\")\n",
        "\n",
        "    x_PCM = x_PCM.detach().cpu().numpy()\n",
        "    y_PCM = y_PCM.detach().cpu().numpy()\n",
        "    t_PCM = t_PCM.detach().cpu().numpy()\n",
        "    T_PCM = T_PCM.detach().cpu().numpy()\n",
        "\n",
        "    x_HS = x_HS.detach().cpu().numpy()\n",
        "    y_HS = y_HS.detach().cpu().numpy()\n",
        "    t_HS = t_HS.detach().cpu().numpy()\n",
        "    T_HS = T_HS.detach().cpu().numpy()\n",
        "\n",
        "    T_disp = []\n",
        "    X_disp = []\n",
        "    Y_disp = []\n",
        "    cnt_PCM = 0\n",
        "    cnt_HS = 0\n",
        "\n",
        "    for i in range(N_t_test):\n",
        "        x_tp = []\n",
        "        y_tp = []\n",
        "        T_tp = []\n",
        "        F_tp = []\n",
        "\n",
        "        for j in range(n_PCM):  # n = number of spatial points per time step (assumed same for PCM and HS)\n",
        "            # PCM Points\n",
        "            x_tp.append(x_PCM[cnt_PCM])\n",
        "            y_tp.append(y_PCM[cnt_PCM])\n",
        "            T_tp.append(T_PCM[cnt_PCM])\n",
        "            cnt_PCM = cnt_PCM + 1\n",
        "\n",
        "        for j in range(n_HS):\n",
        "            x_tp.append(x_HS[cnt_HS])\n",
        "            y_tp.append(y_HS[cnt_HS])\n",
        "            T_tp.append(T_HS[cnt_HS])\n",
        "            cnt_HS = cnt_HS + 1\n",
        "        T_disp.append(T_tp)\n",
        "        X_disp.append(x_tp)\n",
        "        Y_disp.append(y_tp)\n",
        "\n",
        "    # Time step interval\n",
        "    del_t = (t_f - t_i) / (N_t_test - 1)\n",
        "    c_max = 0.3\n",
        "\n",
        "    #### Plot temperature contours #####\n",
        "    for k in range(N_t_test):\n",
        "      if k % 1 == 0:\n",
        "        plt.figure(figsize=(5, 4))\n",
        "        sc = plt.scatter(X_disp[k], Y_disp[k], c=T_disp[k], cmap=plt.cm.jet)\n",
        "        plt.xlim(x_l - 0.001, x_r + 0.001)\n",
        "        plt.ylim(y_b - 0.001, y_t + 0.001)\n",
        "        plt.colorbar(sc)\n",
        "        plt.title('t = ' + str(round(k * del_t, 3)) + ', Temperature plot')\n",
        "        filename = os.path.join(current_dir, f\"plot_{k*del_t:.2f}.png\")\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBFLmfnMmQ-a",
        "outputId": "8d438885-7bc3-4057-9973-1603b1b3fad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k1 =  0.5555555555555556  k2 =  0.03875968992248062  s_norm =  0.4166666666666667\n",
            "ANN(\n",
            "  (tanh): Tanh()\n",
            "  (act): RBF()\n",
            "  (fc_11): Sequential(\n",
            "    (0): Linear(in_features=3, out_features=25, bias=True)\n",
            "    (1): RBF()\n",
            "    (2): Linear(in_features=25, out_features=25, bias=True)\n",
            "    (3): RBF()\n",
            "    (4): Linear(in_features=25, out_features=25, bias=True)\n",
            "  )\n",
            "  (fc_12): Sequential(\n",
            "    (0): Linear(in_features=25, out_features=25, bias=True)\n",
            "    (1): RBF()\n",
            "    (2): Linear(in_features=25, out_features=25, bias=True)\n",
            "    (3): RBF()\n",
            "    (4): Linear(in_features=25, out_features=25, bias=True)\n",
            "  )\n",
            "  (fc_14): Sequential(\n",
            "    (0): Linear(in_features=25, out_features=25, bias=True)\n",
            "    (1): RBF()\n",
            "    (2): Linear(in_features=25, out_features=25, bias=True)\n",
            "    (3): RBF()\n",
            "    (4): Linear(in_features=25, out_features=25, bias=True)\n",
            "  )\n",
            "  (fc_13): Sequential(\n",
            "    (0): RBF()\n",
            "    (1): Linear(in_features=25, out_features=1, bias=True)\n",
            "    (2): RBF()\n",
            "  )\n",
            ")\n",
            "Total trainable parameters in the model: 5326\n"
          ]
        }
      ],
      "source": [
        "#### material params:- RT-35, aluminum ####\n",
        "k_therm = 0.2\n",
        "L = 160000\n",
        "Cp = 2000\n",
        "rho = 800\n",
        "\n",
        "T_solidus = 305\n",
        "T_liquidus = 311\n",
        "\n",
        "#### Heat source ####\n",
        "s = 5000\n",
        "\n",
        "#### Normalising coeffs/ actual domain and temporal simulation params ####\n",
        "T_change = 0.2\n",
        "delta_x = 0.03\n",
        "delta_y = 0.03\n",
        "delta_t = 4000\n",
        "T_sc = (T_liquidus - T_solidus)/T_change\n",
        "\n",
        "#### Normalised Coefficients  ####\n",
        "k1 = k_therm/(rho*Cp*delta_x**2)*delta_t\n",
        "k2 = k_therm/(rho*(Cp+L/(T_liquidus - T_solidus))*delta_x**2)*delta_t\n",
        "s_norm = s*delta_t/(rho*Cp*T_sc)\n",
        "\n",
        "#### FVM Domain Parameters ####\n",
        "N_x = 66\n",
        "N_y = 66\n",
        "N_bc = 66\n",
        "x_l = 0\n",
        "y_b = 0\n",
        "x_r = 1\n",
        "y_t = 1\n",
        "t_i = 0\n",
        "t_f = 1.25\n",
        "N_t = int(t_f*100)+1\n",
        "\n",
        "#### Circular heat source ####\n",
        "x_c = 0.5\n",
        "y_c = 0.5\n",
        "r = 0.3\n",
        "\n",
        "print('k1 = ',k1,' k2 = ', k2, ' s_norm = ', s_norm)\n",
        "\n",
        "\n",
        "# Training data and initial data\n",
        "layer_size_1 = [3, 25, 25, 1]\n",
        "model = ANN(layer_size_1).to(device)\n",
        "# model = ANN(layer_size_1)\n",
        "print(model)\n",
        "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Total trainable parameters in the model:\", total_trainable_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "KvqO8unXmSun",
        "outputId": "646144ca-7e39-483d-a33a-0e76c49a140c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N_t =  126\n",
            "t_f =  1.25\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-627148899.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m                                                                                      )\n\u001b[1;32m     63\u001b[0m     \u001b[0moptimiser1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0moptimiser1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# Lists for storing\n",
        "loss_store = []\n",
        "lr1 = 1e-5\n",
        "optimiser1 = torch.optim.Rprop(model.parameters(), lr=lr1)\n",
        "epochs = 10000\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimiser1, T_max=epochs)\n",
        "\n",
        "# Load model\n",
        "model.train()\n",
        "fileName = 'radialsource2.pth'\n",
        "save_path = \"/content/drive/MyDrive/NNOutputs/\" + fileName\n",
        "load_model(model, save_path)\n",
        "# freeze(model)\n",
        "current_dir = \"/content/drive/MyDrive/NNOutputs\"\n",
        "\n",
        "# Loss function weights\n",
        "# w1 = 1\n",
        "# w2 = 1\n",
        "# w3 = 20\n",
        "# w4 = 20\n",
        "# w5 = 20\n",
        "# w6 = 20\n",
        "# w7 = 1\n",
        "# w8 = 1\n",
        "\n",
        "w1 = 1\n",
        "w2 = 1\n",
        "w3 = 1\n",
        "w4 = 1\n",
        "w5 = 1\n",
        "w6 = 1\n",
        "w7 = 50\n",
        "w8 = 50\n",
        "\n",
        "print('N_t = ', N_t)\n",
        "print('t_f = ', t_f)\n",
        "\n",
        "# Initial conditions\n",
        "x_PCM, y_PCM, t_PCM, x_HS, y_HS, t_HS, start_PCM, start_HS, n_PCM, n_HS, dist= xyt_train_data(N_x, x_l, x_r, N_y, y_b, y_t, N_t, t_i, t_f, N_bc, x_c, y_c, r)\n",
        "N_t_test = int(t_f*4 + 1)\n",
        "x_PCM_test, y_PCM_test, t_PCM_test, x_HS_test, y_HS_test, t_HS_test, start_PCM_test, start_HS_test, n_PCM_test, n_HS_test, _ = xyt_train_data(N_x, x_l, x_r, N_y, y_b, y_t, N_t_test, t_i, t_f, N_bc, x_c, y_c, r)\n",
        "\n",
        "x_PCM = x_PCM.to(device).requires_grad_()\n",
        "y_PCM = y_PCM.to(device).requires_grad_()\n",
        "t_PCM = t_PCM.to(device).requires_grad_()\n",
        "x_HS = x_HS.to(device).requires_grad_()\n",
        "y_HS = y_HS.to(device).requires_grad_()\n",
        "t_HS = t_HS.to(device).requires_grad_()\n",
        "\n",
        "x_PCM_test = x_PCM_test.cpu()\n",
        "y_PCM_test = y_PCM_test.cpu()\n",
        "t_PCM_test = t_PCM_test.cpu()\n",
        "x_HS_test = x_HS_test.cpu()\n",
        "y_HS_test = y_HS_test.cpu()\n",
        "t_HS_test = t_HS_test.cpu()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    loss, eq_PCM, eq_HS, bc1, bc2, bc3, bc4, ic_PCM, ic_HS = get_loss(x_PCM, y_PCM, t_PCM, x_HS, y_HS, t_HS, start_PCM,\n",
        "                                                                                     start_HS, k1, k2, 1, w1, w2, w3, w4, w5, w6, w7,\n",
        "                                                                                     w8, s_norm, n_PCM, n_HS, T_change, N_t, model, 1\n",
        "                                                                                     )\n",
        "    optimiser1.zero_grad()\n",
        "    loss.backward()\n",
        "    optimiser1.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if epoch%1==0:\n",
        "        logging.info(f\"Epoch = {epoch}, Total Loss = {loss.detach().cpu().numpy():.6f}, eq_PCM = {eq_PCM.detach().cpu().numpy():.6f}, eq_HS = {eq_HS.detach().cpu().numpy():.6f}, BC1 (left) = {bc1.detach().cpu().numpy():.6f}, BC2 (bottom) = {bc2.detach().cpu().numpy():.6f}, BC3 (top) = {bc3.detach().cpu().numpy():.6f}, BC4 (right) = {bc4.detach().cpu().numpy():.6f}, ic_PCM = {ic_PCM.detach().cpu().numpy():.6f}, ic_HS = {ic_HS.detach().cpu().numpy():.6f}\")\n",
        "        del loss, eq_PCM, eq_HS, bc1, bc2, bc3, bc4  # delete unnecessary tensors\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if epoch%20==0:\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        logging.info(f\"epoch = {epoch}, saved\")\n",
        "        plotcheck(x_PCM_test, y_PCM_test, t_PCM_test, x_HS_test, y_HS_test, t_HS_test, start_PCM_test, start_HS_test, n_PCM_test, n_HS_test, N_t_test, model, T_change, current_dir)\n",
        "        model = model.to('cuda')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
