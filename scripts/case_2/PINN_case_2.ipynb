{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22731c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import time\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a588bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_torch(arr):\n",
    "    \n",
    "    arr = torch.FloatTensor(arr)\n",
    "    arr = arr.unsqueeze(-1)\n",
    "    arr = arr.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def xyt_train_data(N_x, x_l, x_r, N_y, y_b, y_t, N_t, t_i, t_f, N_bc):\n",
    "\n",
    "    start = []\n",
    "    \n",
    "    ####  Spatial array\n",
    "    x_r_train = 1*x_r\n",
    "    y_t_train = 1*y_t\n",
    "    N_x_train = int(x_r_train/x_r*N_x) \n",
    "    N_y_train = int(y_t_train/y_t*N_y) \n",
    "    \n",
    "    x_train = np.linspace(x_l, x_r_train, N_x_train)  \n",
    "    y_train = np.linspace(y_b, y_t, N_y)\n",
    "    x_train = np.repeat(x_train, N_y)\n",
    "    y_train = np.tile(y_train, N_x_train)\n",
    "    \n",
    "    n = x_train.shape[0]\n",
    "    x_train = np.tile(x_train, N_t)\n",
    "    y_train = np.tile(y_train, N_t)\n",
    "    t_train = np.linspace(t_i, t_f, N_t)\n",
    "    t_train = np.repeat(t_train, n )\n",
    "    \n",
    "    start.append( 0 )\n",
    "    start.append( t_train.shape[0] ) \n",
    "    print('n = ', n)\n",
    "    print('training data-size = ', t_train.shape[0] )\n",
    "    \n",
    "    # Boundary collocation points\n",
    "    x_bc1 = np.ones((N_bc))*x_l\n",
    "    y_bc1 = np.linspace(y_b, y_t, N_bc)\n",
    "    t_bc1 = np.linspace(t_i+0.005,t_f,N_t)\n",
    "    x_bc1 = np.tile(x_bc1, N_t)\n",
    "    y_bc1 = np.tile(y_bc1, N_t)\n",
    "    t_bc1 = np.repeat(t_bc1, N_bc)\n",
    "    \n",
    "    start.append( start[-1] + t_bc1.shape[0] )\n",
    "    \n",
    "    x_bc2 = np.ones((N_bc))*x_r\n",
    "    y_bc2 = np.linspace(y_b+0.001,y_t,N_bc)\n",
    "    t_bc2 = np.linspace(t_i+0.005,t_f,N_t)\n",
    "    x_bc2 = np.tile(x_bc2, N_t)\n",
    "    y_bc2 = np.tile(y_bc2, N_t)\n",
    "    t_bc2 = np.repeat(t_bc2, N_bc)\n",
    "    \n",
    "    start.append( start[-1] + t_bc2.shape[0] )\n",
    "    \n",
    "    x_bc3 = np.linspace(x_l,x_r,N_bc)\n",
    "    y_bc3 = np.ones((N_bc))*y_b\n",
    "    t_bc3 = np.linspace(t_i+0.005,t_f,N_t)\n",
    "    x_bc3 = np.tile(x_bc3, N_t)\n",
    "    y_bc3 = np.tile(y_bc3, N_t)\n",
    "    t_bc3 = np.repeat(t_bc3, N_bc)\n",
    "    \n",
    "    start.append( start[-1] + t_bc3.shape[0] )\n",
    "    \n",
    "    x_bc4 = np.linspace(x_l+0.001,x_r,N_bc)\n",
    "    y_bc4 = np.ones((N_bc))*y_t\n",
    "    t_bc4 = np.linspace(t_i+0.005,t_f,N_t)\n",
    "    x_bc4 = np.tile(x_bc4, N_t)\n",
    "    y_bc4 = np.tile(y_bc4, N_t)\n",
    "    t_bc4 = np.repeat(t_bc4, N_bc)\n",
    "    \n",
    "    start.append( start[-1] + t_bc4.shape[0] )\n",
    "    \n",
    "    # np to torch\n",
    "    x_T = np.concatenate((x_train, x_bc1, x_bc2, x_bc3, x_bc4), axis = 0)\n",
    "    y_T = np.concatenate((y_train, y_bc1, y_bc2, y_bc3, y_bc4), axis = 0)\n",
    "    t_T = np.concatenate((t_train, t_bc1, t_bc2, t_bc3, t_bc4), axis = 0)\n",
    "    x_T = np_to_torch(x_T)\n",
    "    y_T = np_to_torch(y_T)\n",
    "    t_T = np_to_torch(t_T)\n",
    "    \n",
    "    return x_T, y_T, t_T, start, n\n",
    "\n",
    "class RBF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        y = 1.2*torch.exp(-5*torch.square(x)) - 0.1\n",
    "        return y\n",
    "\n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        init.xavier_normal_(m.bias)\n",
    "    \n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, layer_size_1):\n",
    "        super(ANN, self).__init__()\n",
    "\n",
    "        self.act = RBF()\n",
    "        \n",
    "        ########### Fully conected model-1 ###########\n",
    "        modules_11 = []\n",
    "        modules_11.append(nn.Linear(layer_size_1[0], layer_size_1[1]))  \n",
    "        modules_11.append(self.act)\n",
    "        modules_11.append(nn.Linear(layer_size_1[1], layer_size_1[2]))  \n",
    "        self.fc_11 = nn.Sequential( *modules_11 )\n",
    "                \n",
    "        modules_12 = []\n",
    "        modules_12.append(nn.Linear(layer_size_1[1], layer_size_1[2])) \n",
    "        modules_12.append(self.act)\n",
    "        modules_12.append(nn.Linear(layer_size_1[1], layer_size_1[2])) \n",
    "        modules_12.append(self.act)\n",
    "        self.fc_12 = nn.Sequential( *modules_12 )\n",
    "        \n",
    "        modules_13 = []\n",
    "        modules_13.append(nn.Linear(layer_size_1[2], layer_size_1[3])) \n",
    "        self.fc_13 = nn.Sequential( *modules_13 )\n",
    "\n",
    "        for layer in [self.fc_11.modules(), self.fc_12.modules(), self.fc_13.modules()]:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                xavier_init(layer)\n",
    "\n",
    "    def forward(self, x_T, y_T, t_T, start, result, pr):\n",
    "\n",
    "        ########### Fully conected model-1 ###########\n",
    "        out_1 = self.fc_11( torch.cat((x_T, y_T, t_T),1) )\n",
    "        out_2 = self.fc_12( out_1 )\n",
    "        T = self.fc_13( out_1 + out_2 )\n",
    "        \n",
    "        dTdx = torch.autograd.grad(T, x_T, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "        d2Tdx2 = torch.autograd.grad(dTdx, x_T, grad_outputs=torch.ones_like(dTdx), create_graph=True)[0]\n",
    "        dTdy = torch.autograd.grad(T, y_T, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "        d2Tdy2 = torch.autograd.grad(dTdy, y_T, grad_outputs=torch.ones_like(dTdy), create_graph=True)[0]\n",
    "        dTdt = torch.autograd.grad(T, t_T, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "\n",
    "        return  T, dTdx, dTdy, d2Tdx2, d2Tdy2, dTdt\n",
    "    \n",
    "def get_loss(x_T, y_T, t_T, start, k1, k2, w1, w2, w3, w4, w5, w6, w7, q_norm, n, T_change, N_t):\n",
    " \n",
    "    T, dTdx, dTdy, d2Tdx2, d2Tdy2, dTdt = model( x_T, y_T, t_T, start, 0, 0)\n",
    "\n",
    "    eq1 = w1*( torch.sum( torch.mul( torch.square( dTdt - k1*(d2Tdx2 + d2Tdy2)), torch.where(T>=T_change,1,0) ) ) \n",
    "    + torch.sum( torch.mul( torch.square( dTdt - k2*(d2Tdx2 + d2Tdy2)), torch.where(T<T_change,1,0) ) ) )/(n*N_t)\n",
    "    bc1 = w2*torch.sum( torch.square( dTdx[ start[1]:start[2] ] + q_norm ) )/(start[2]-start[1]) #left boundary\n",
    "    bc2 = w3*torch.sum( torch.square( dTdy[ start[3]:start[4] ] + q_norm  ) )/(start[4]-start[3]) #bottom boundary\n",
    "    bc3 = w4*torch.sum( torch.square( dTdy[ start[4]:start[5] ] ) )/(start[5]-start[4]) #top boundary\n",
    "    bc4 = w5*torch.sum( torch.square( dTdx[ start[2]:start[3] ] ) )/(start[3]-start[2]) #right boundary\n",
    "    ic1 = w6*torch.sum( torch.square( T[0:n] )  )/(n) \n",
    "    \n",
    "    loss = eq1 + bc1 + bc2 + bc3 + bc4 + ic1 \n",
    "    \n",
    "    return loss, eq1, bc1, bc2, bc3, bc4, ic1\n",
    "\n",
    "def print_loss(epoch, loss, eq1, bc1, bc2, bc3, bc4, ic1):\n",
    "    print('epoch = ',epoch)\n",
    "    print('loss = ',loss.detach().numpy())\n",
    "    print('eq1_loss = ',eq1.detach().numpy())\n",
    "    print('bc1_loss = ',bc1.detach().numpy())\n",
    "    print('bc2_loss = ',bc2.detach().numpy())\n",
    "    print('bc3_loss = ',bc3.detach().numpy())\n",
    "    print('bc4_loss = ',bc4.detach().numpy())\n",
    "    print('ic1_loss = ',ic1.detach().numpy())\n",
    "\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3fe607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### material params:- RT-35 ####\n",
    "k_therm = 0.2\n",
    "L = 160000\n",
    "Cp = 2000\n",
    "rho = 800 \n",
    "T_solidus = 305\n",
    "T_liquidus = 311\n",
    "\n",
    "#### Heat flux ####\n",
    "q = 225\n",
    "\n",
    "#### Normalising coeffs ####\n",
    "T_change = 0.35\n",
    "delta_x = 0.02\n",
    "delta_y = 0.02\n",
    "delta_T = 17.14\n",
    "delta_t = 2400\n",
    "\n",
    "#### Normalised Coefficients  ####\n",
    "k1 = k_therm/(rho*Cp*delta_x**2)*delta_t\n",
    "k2 = k_therm/(rho*(Cp+L/(T_liquidus - T_solidus))*delta_x**2)*delta_t\n",
    "q_norm = q*delta_x/(k_therm*delta_T)\n",
    "\n",
    "#### Neural Network Parameters ####\n",
    "layer_size_1 = [3, 20, 20, 1]\n",
    "model = ANN(layer_size_1)\n",
    "\n",
    "#### optimizer and scheduler ####\n",
    "lr1 = 1e-3\n",
    "optimiser1 = torch.optim.Rprop(model.parameters(), lr=lr1)\n",
    "epochs = 10000\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimiser1, T_max=epochs)\n",
    "\n",
    "#### Collocation points ####\n",
    "x_l = 0\n",
    "y_b = 0\n",
    "x_r = 1\n",
    "y_t = 1\n",
    "t_i = 0\n",
    "t_f = 2.4\n",
    "N_x = 66\n",
    "N_y = 66\n",
    "N_t = 200\n",
    "N_bc = 66\n",
    "x_T, y_T, t_T, start, n = xyt_train_data(N_x, x_l, x_r, N_y, y_b, y_t, N_t, t_i, t_f, N_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33e2f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model.train()  \n",
    "\n",
    "# Loss function weights\n",
    "w1 = 1\n",
    "w2 = 1\n",
    "w3 = 1\n",
    "w4 = 1\n",
    "w5 = 1\n",
    "w6 = 50\n",
    "w7 = 1\n",
    "\n",
    "for epoch in range(epochs):        \n",
    "    #Backpropogation and optimisation\n",
    "    w7 = 1\n",
    "    loss, eq1, bc1, bc2, bc3, bc4, ic1 =  get_loss(x_T, y_T, t_T, start, k1, k2, w1, w2, w3, w4, w5, w6, w7, q_norm, n, T_change, N_t)\n",
    "    optimiser1.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser1.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch%1==0:\n",
    "        print_loss(epoch, loss, eq1, bc1, bc2, bc3, bc4, ic1)\n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
